# Correcciones de Streaming Backend

## Problemas Corregidos

### 1. ‚ùå Error: "Object of type SessionStateSchema is not JSON serializable"

**Causa**: El objeto `SessionStateSchema` (Pydantic model) no se puede serializar directamente a JSON en los eventos SSE.

**Soluci√≥n**: 
- Convertir el schema a dict antes de serializarlo: `session_obj.dict()`
- Reconstruir el schema desde el dict antes de guardarlo en la DB

**Archivos modificados**:
- `app/api/v1/endpoints/ai_chat.py` l√≠neas 207-214 y 223-232

```python
# Antes
event_data = json.dumps(event, ensure_ascii=False)

# Despu√©s
if event["type"] == "complete" and "session" in event["data"]:
    session_obj = event["data"]["session"]
    if hasattr(session_obj, 'dict'):
        event["data"]["session"] = session_obj.dict()
        
event_data = json.dumps(event, ensure_ascii=False)
```

### 2. ‚ùå Error: "404 models/gemini-1.5-flash is not found for API version v1beta"

**Causa**: El modelo `gemini-1.5-flash` no existe o no est√° disponible en la API de Google.

**Soluci√≥n**: Actualizar todas las referencias a `gemini-2.0-flash-exp` que es el modelo correcto.

**Archivos modificados**:
- `app/services/ai_service.py` - 8 instancias actualizadas:
  1. Docstring (l√≠nea 6)
  2. Modelo por defecto (l√≠nea 45)
  3. Funci√≥n `guardrail_check` (l√≠nea 147)
  4. Funci√≥n `extract_slots_with_llm` (l√≠nea 289)
  5. Funci√≥n `handle_user_turn` (l√≠nea 602)
  6. Funci√≥n `handle_user_turn_streaming` (l√≠nea 821)
  7. Funci√≥n `generate_chat_response` (l√≠nea 932)
  8. Funci√≥n `generate_profile_summary` (l√≠nea 957)

```python
# Antes
model = genai.GenerativeModel('gemini-1.5-flash')

# Despu√©s
model = genai.GenerativeModel('gemini-2.0-flash-exp')
```

### 3. ‚ö†Ô∏è Warning: "Error en extracci√≥n LLM, usando heur√≠stica"

**Causa**: El modelo incorrecto causaba que las extracciones LLM fallaran.

**Resultado**: Con el modelo correcto (`gemini-2.0-flash-exp`), las extracciones deber√≠an funcionar correctamente ahora.

---

## Archivos Modificados

1. **app/services/ai_service.py**
   - ‚úÖ 8 referencias actualizadas de `gemini-1.5-flash` ‚Üí `gemini-2.0-flash-exp`
   - ‚úÖ Todas las funciones usan el modelo correcto

2. **app/api/v1/endpoints/ai_chat.py**
   - ‚úÖ Serializaci√≥n correcta de SessionStateSchema a dict
   - ‚úÖ Reconstrucci√≥n del schema para guardado en DB

---

## Testing

### Verificaci√≥n Manual

```bash
# 1. Verificar que no quedan referencias al modelo antiguo
grep -r "gemini-1.5-flash" app/

# 2. Probar el endpoint de streaming
curl -X POST http://localhost:8000/api/v1/ai-chat/send-stream \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"message": "Hola Flou, necesito ayuda con mi ensayo"}' \
  --no-buffer
```

### Resultados Esperados

‚úÖ Sin errores de modelo no encontrado  
‚úÖ Sin errores de serializaci√≥n JSON  
‚úÖ Streaming funciona correctamente  
‚úÖ Sesi√≥n se guarda correctamente en la DB  
‚úÖ Mensajes se guardan correctamente  

---

## Logs Esperados (despu√©s de la correcci√≥n)

```
INFO: streaming_request_start
INFO: crisis_check_negative
INFO: slots_extracted (sin warnings)
INFO: strategy_generated
INFO: chunk enviado
INFO: chunk enviado
...
INFO: complete enviado
INFO: sesi√≥n guardada correctamente
```

**Sin errores de**:
- ‚ùå "404 models/gemini-1.5-flash is not found"
- ‚ùå "Object of type SessionStateSchema is not JSON serializable"
- ‚ö†Ô∏è "Error en extracci√≥n LLM, usando heur√≠stica"

---

## Impacto en el Frontend

El frontend ahora deber√≠a recibir los eventos correctamente sin el error "Response body is null".

Los eventos SSE llegar√°n en el formato correcto:
```
data: {"type":"metadata","data":{...}}
data: {"type":"chunk","data":{"text":"Hola"}}
data: {"type":"chunk","data":{"text":" ¬øc√≥mo"}}
data: {"type":"complete","data":{"session":{...},"quick_replies":[...]}}
```

---

## Pr√≥ximos Pasos

1. ‚úÖ Hacer commit de los cambios
2. ‚úÖ Deploy a Azure
3. üß™ Probar en la app m√≥vil
4. üìä Monitorear logs para verificar que no hay m√°s errores
